{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0143fa28-c231-46fa-9f57-24fe46145df2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%store -r params\n",
    "%store -r secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f75f57b-a2cb-48e1-872e-5b15748f77ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.avro.functions import to_avro, from_avro\n",
    "from pyspark.sql.functions import col, struct, lit, unbase64\n",
    "from pyspark.sql.types import IntegerType\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58dc55d7-ce70-4452-977e-eebaae6a27dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/31 13:22:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Tokenization\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba8f8ee-6ab5-4186-abd8-8fa693eb4656",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(params[\"avro_schema_file\"]) as f:\n",
    "    avsc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "288a0f52-1117-4e03-b966-8b4a820c4518",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_checkpoint_location = params[\"checkpoints\"][\"tokenization_read\"]\n",
    "os.makedirs(read_checkpoint_location, exist_ok=True)\n",
    "write_checkpoint_location = params[\"checkpoints\"][\"tokenization_write\"]\n",
    "os.makedirs(write_checkpoint_location, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bbf2e83-0819-4967-92ec-90e22cd1b038",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/31 13:23:42 WARN KafkaSourceProvider: Kafka option 'kafka.group.id' has been set on this query, it is\n",
      " not recommended to set this option. This option is unsafe to use since multiple concurrent\n",
      " queries or sources using the same group id will interfere with each other as they are part\n",
      " of the same consumer group. Restarted queries may also suffer interference from the\n",
      " previous run having the same group id. The user should have only one query per group id,\n",
      " and/or set the option 'kafka.session.timeout.ms' to be very small so that the Kafka\n",
      " consumers from the previous query are marked dead by the Kafka group coordinator before the\n",
      " restarted query starts running.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "raw_avro_df = (spark.readStream\n",
    "          .format(\"kafka\")\n",
    "          .option(\"kafka.bootstrap.servers\", secrets[\"RED_KAFKA_SERVERS\"])\n",
    "          .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "          .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "          .option(\"kafka.sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(secrets[\"RED_KAFKA_USERNAME\"], secrets[\"RED_KAFKA_PASSWORD\"]))\n",
    "          .option(\"subscribe\", params[\"topic\"][\"red_landing\"])\n",
    "          .option(\"kafka.group.id\", \"tokenization\")\n",
    "          .option('checkpointLocation', read_checkpoint_location)\n",
    "          .load()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c6d6a7-323a-4835-8921-665f7db2c0ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_json_df = raw_avro_df.select(\"key\",\"value\").withColumn(\"value\", from_avro(\"value\", avsc))\n",
    "tokenized_json_df = raw_json_df.withColumn(\"value\", col(\"value\").withField(\"cc_num\",lit(random.randrange(1000000000000000,9999999999999999))))\n",
    "tokenized_avro_df = tokenized_json_df.withColumn(\"value\",to_avro(col(\"value\"), avsc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a1ef2af-4288-4811-981e-037846a3a1b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/31 13:25:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/31 13:25:08 WARN KafkaSourceProvider: Kafka option 'kafka.group.id' has been set on this query, it is\n",
      " not recommended to set this option. This option is unsafe to use since multiple concurrent\n",
      " queries or sources using the same group id will interfere with each other as they are part\n",
      " of the same consumer group. Restarted queries may also suffer interference from the\n",
      " previous run having the same group id. The user should have only one query per group id,\n",
      " and/or set the option 'kafka.session.timeout.ms' to be very small so that the Kafka\n",
      " consumers from the previous query are marked dead by the Kafka group coordinator before the\n",
      " restarted query starts running.\n",
      "    \n",
      "24/03/31 13:25:08 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, group.id, auto.offset.reset]' were supplied but are not used yet.\n",
      "24/03/31 13:25:19 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "sQuery = (tokenized_avro_df\n",
    "        .writeStream\n",
    "        .format(\"kafka\")\n",
    "        .queryName(\"tokenization\")\n",
    "        .option(\"kafka.bootstrap.servers\", secrets[\"GREEN_KAFKA_SERVERS\"])\n",
    "        .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "        .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "        .option(\"kafka.sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(secrets[\"GREEN_KAFKA_USERNAME\"], secrets[\"GREEN_KAFKA_PASSWORD\"]))\n",
    "        .option(\"topic\",params[\"topic\"][\"green_landing\"])\n",
    "        .option(\"checkpointLocation\",write_checkpoint_location)\n",
    "        .outputMode(\"append\")\n",
    "        .start()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "tokenization",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

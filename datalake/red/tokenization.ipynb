{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0143fa28-c231-46fa-9f57-24fe46145df2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%store -r params\n",
    "%store -r secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f75f57b-a2cb-48e1-872e-5b15748f77ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.avro.functions import to_avro, from_avro\n",
    "from pyspark.sql.functions import col, struct, lit, unbase64, udf\n",
    "from pyspark.sql.types import IntegerType, StringType, LongType\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58dc55d7-ce70-4452-977e-eebaae6a27dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark = SparkSession.getActiveSession()\n",
    "#spark = spark if spark else SparkSession.builder.appName(\"Tokenization\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba8f8ee-6ab5-4186-abd8-8fa693eb4656",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(params[\"avro_schema_file\"]) as f:\n",
    "    avsc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "288a0f52-1117-4e03-b966-8b4a820c4518",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_checkpoint_location = params[\"checkpoints\"][\"tokenization_read\"]\n",
    "os.makedirs(read_checkpoint_location, exist_ok=True)\n",
    "write_checkpoint_location = params[\"checkpoints\"][\"tokenization_write\"]\n",
    "os.makedirs(write_checkpoint_location, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bbf2e83-0819-4967-92ec-90e22cd1b038",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 14:35:27 WARN KafkaSourceProvider: Kafka option 'kafka.group.id' has been set on this query, it is\n",
      " not recommended to set this option. This option is unsafe to use since multiple concurrent\n",
      " queries or sources using the same group id will interfere with each other as they are part\n",
      " of the same consumer group. Restarted queries may also suffer interference from the\n",
      " previous run having the same group id. The user should have only one query per group id,\n",
      " and/or set the option 'kafka.session.timeout.ms' to be very small so that the Kafka\n",
      " consumers from the previous query are marked dead by the Kafka group coordinator before the\n",
      " restarted query starts running.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "raw_avro_df = (spark.readStream\n",
    "          .format(\"kafka\")\n",
    "          .option(\"kafka.bootstrap.servers\", secrets[\"RED_KAFKA_SERVERS\"])\n",
    "          .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "          .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "          .option(\"kafka.sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(secrets[\"RED_KAFKA_USERNAME\"], secrets[\"RED_KAFKA_PASSWORD\"]))\n",
    "          .option(\"subscribe\", params[\"topic\"][\"red_landing\"])\n",
    "          .option(\"kafka.group.id\", \"tokenization\")\n",
    "          .option('checkpointLocation', read_checkpoint_location)\n",
    "          .load()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def tokenize_cc_num(cc_num):\n",
    "    last4 = cc_num%10000\n",
    "    add_nums = 1111111111110000\n",
    "    tokenized_cc_num = add_nums + last4\n",
    "    return tokenized_cc_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_df(df):\n",
    "    value_df = df.select(\"value.*\")\n",
    "    tokenized_value_df = value_df.withColumn(\"cc_num\", tokenize_cc_num(col(\"cc_num\")).cast(LongType()))\n",
    "    tokenized_value_kafka = tokenized_value_df.selectExpr(\"id as key\", \"(struct(*)) as value\")\n",
    "    return tokenized_value_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c6d6a7-323a-4835-8921-665f7db2c0ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_json_df = raw_avro_df.select(\"key\",\"value\").withColumn(\"value\", from_avro(\"value\", avsc))\n",
    "tokenized_json_df = tokenize_df(raw_json_df)\n",
    "tokenized_avro_df = tokenized_json_df.withColumn(\"key\", col(\"key\").cast(StringType())).withColumn(\"value\",to_avro(col(\"value\"), avsc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a1ef2af-4288-4811-981e-037846a3a1b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 14:35:28 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "sQuery = (tokenized_avro_df\n",
    "        .writeStream\n",
    "        .format(\"kafka\")\n",
    "        .queryName(\"tokenization\")\n",
    "        .option(\"kafka.bootstrap.servers\", secrets[\"GREEN_KAFKA_SERVERS\"])\n",
    "        .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "        .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "        .option(\"kafka.sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(secrets[\"GREEN_KAFKA_USERNAME\"], secrets[\"GREEN_KAFKA_PASSWORD\"]))\n",
    "        .option(\"topic\",params[\"topic\"][\"green_landing\"])\n",
    "        .option(\"checkpointLocation\",write_checkpoint_location)\n",
    "        .outputMode(\"append\")\n",
    "        .start()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 14:35:29 WARN KafkaSourceProvider: Kafka option 'kafka.group.id' has been set on this query, it is\n",
      " not recommended to set this option. This option is unsafe to use since multiple concurrent\n",
      " queries or sources using the same group id will interfere with each other as they are part\n",
      " of the same consumer group. Restarted queries may also suffer interference from the\n",
      " previous run having the same group id. The user should have only one query per group id,\n",
      " and/or set the option 'kafka.session.timeout.ms' to be very small so that the Kafka\n",
      " consumers from the previous query are marked dead by the Kafka group coordinator before the\n",
      " restarted query starts running.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Initializing sources',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- id: integer (nullable = true)\n",
      " |    |-- trans_date_trans_time: long (nullable = true)\n",
      " |    |-- cc_num: long (nullable = true)\n",
      " |    |-- merchant: string (nullable = true)\n",
      " |    |-- category: string (nullable = true)\n",
      " |    |-- amt: double (nullable = true)\n",
      " |    |-- first: string (nullable = true)\n",
      " |    |-- last: string (nullable = true)\n",
      " |    |-- gender: string (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- zip: integer (nullable = true)\n",
      " |    |-- lat: double (nullable = true)\n",
      " |    |-- long: double (nullable = true)\n",
      " |    |-- city_pop: integer (nullable = true)\n",
      " |    |-- job: string (nullable = true)\n",
      " |    |-- dob: string (nullable = true)\n",
      " |    |-- trans_num: string (nullable = true)\n",
      " |    |-- unix_time: integer (nullable = true)\n",
      " |    |-- merch_lat: double (nullable = true)\n",
      " |    |-- merch_long: double (nullable = true)\n",
      " |    |-- is_fraud: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 14:35:29 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, group.id, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    }
   ],
   "source": [
    "raw_json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "tokenization",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
